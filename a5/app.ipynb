{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from elasticsearch import Elasticsearch \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Pr:\n",
    "\n",
    "    def __init__(self, alpha):\n",
    "        self.crawled_folder = Path(\"resources/crawled\")\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def url_extractor(self):\n",
    "        url_maps = {}\n",
    "        all_urls = set([])\n",
    "\n",
    "        for file in os.listdir(self.crawled_folder):\n",
    "            if file.endswith(\".txt\"):\n",
    "                j = json.load(open(os.path.join(self.crawled_folder, file)))\n",
    "                all_urls.add(j['url'])\n",
    "                for s in j['url_lists']:\n",
    "                    all_urls.add(s)\n",
    "                url_maps[j['url']] = list(set(j['url_lists']))\n",
    "        all_urls = list(all_urls)\n",
    "        return url_maps, all_urls\n",
    "  \n",
    "    def pr_calc(self):\n",
    "        url_maps, all_urls = self.url_extractor()\n",
    "        url_matrix = pd.DataFrame(columns=all_urls, index=all_urls)\n",
    "\n",
    "        for url in url_maps:\n",
    "            if len(url_maps[url]) > 0 and len(all_urls) > 0:\n",
    "                url_matrix.loc[url] = (1 - self.alpha) * (1 / len(all_urls))\n",
    "                url_matrix.loc[url, url_maps[url]] = url_matrix.loc[url, url_maps[url]] + (self.alpha * (1 / len(url_maps[url])))\n",
    "\n",
    "        url_matrix.loc[url_matrix.isnull().all(axis=1), :] = (1 / len(all_urls))\n",
    "        x0 = np.matrix([1 / len(all_urls)] * len(all_urls))\n",
    "        P = np.asmatrix(url_matrix.values)\n",
    "\n",
    "        prev_Px = x0\n",
    "        Px = x0 * P\n",
    "        i = 0\n",
    "        while (any(abs(np.asarray(prev_Px).flatten() - np.asarray(Px).flatten()) > 1e-8)):\n",
    "            i += 1\n",
    "            prev_Px = Px\n",
    "            Px = Px * P\n",
    "\n",
    "        print('Converged in {0} iterations: {1}'.format(i, np.around(np.asarray(Px).flatten().astype(float), 5)))\n",
    "\n",
    "        self.pr_result = pd.DataFrame(Px, columns=url_matrix.index, index=['score']).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class IndexerWithPR:\n",
    "    def __init__(self):\n",
    "        self.crawled_folder = Path('C:/Users/user/Documents/Year3_2/IR/resources/crawled')\n",
    "        with open(self.crawled_folder / 'url_list.pickle', 'rb') as f:\n",
    "            self.file_mapper = pickle.load(f)\n",
    "        self.es_client = Elasticsearch(\"https://localhost:9200\", \n",
    "                                       basic_auth=(\"elastic\", \"ySGo56ThrQ2moHl+WbG2\"), \n",
    "                                       ca_certs=\"~/http_ca.crt\")\n",
    "        with open('pickled/pr_instance.pkl', 'rb') as f:\n",
    "            self.pr = pickle.load(f)\n",
    "        self.indexed_data_path = \"indexed_data.pkl\"\n",
    "\n",
    "    def run_indexer(self):\n",
    "        self.es_client.options(ignore_status=[400, 404]).indices.delete(index='simple')\n",
    "        self.es_client.options(ignore_status=[400]).indices.create(index='simple')\n",
    "\n",
    "        if os.path.exists(self.indexed_data_path):\n",
    "            print(\"Loading indexed data from pickle file...\")\n",
    "            with open(self.indexed_data_path, \"rb\") as f:\n",
    "                indexed_data = pickle.load(f)\n",
    "        else:\n",
    "            print(\"Processing files and creating indexed data...\")\n",
    "            indexed_data = []\n",
    "\n",
    "            for file in os.listdir(self.crawled_folder):\n",
    "                if file.endswith(\".txt\"):\n",
    "                    with open(os.path.join(self.crawled_folder, file), 'r') as f:\n",
    "                        j = json.load(f)\n",
    "                    j['id'] = j['url']\n",
    "                    j['pagerank'] = self.pr.pr_result.loc[j['id']].score\n",
    "                    indexed_data.append(j)\n",
    "\n",
    "            # Save processed data to a pickle file\n",
    "            with open(self.indexed_data_path, \"wb\") as f:\n",
    "                pickle.dump(indexed_data, f)\n",
    "        \n",
    "        # Send data to Elasticsearch\n",
    "        for doc in indexed_data:\n",
    "            self.es_client.index(index='simple', body=doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = IndexerWithPR()\n",
    "indexer.run_indexer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/search_es_pr', methods=['GET'])\n",
    "def search_es_pr():\n",
    "    start = time.time()\n",
    "    query_term = request.args.get('query', '')\n",
    "\n",
    "    results = app.es_client.search(\n",
    "        index='simple',\n",
    "        source_excludes=['url_lists'],\n",
    "        size=100,\n",
    "        query={\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match\": {\"text\": query_term}},\n",
    "                \"script\": {\"source\": \"_score * doc['pagerank'].value\"}\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    total_hit = results['hits']['total']['value']\n",
    "\n",
    "    # Extract relevant fields\n",
    "    results_list = [\n",
    "        {\n",
    "            'title': hit[\"_source\"]['title'],\n",
    "            'url': hit[\"_source\"]['url'],\n",
    "            'text': hit[\"_source\"]['text'][:100]  # First 100 characters of text\n",
    "        }\n",
    "        for hit in results['hits']['hits']\n",
    "    ]\n",
    "\n",
    "    return render_template('search_results.html', query=query_term, results=results_list, total_hit=total_hit, elapse=end - start)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from elasticsearch import Elasticsearch \n",
    "from flask import Flask,request\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class TFIDFRanker:\n",
    "    def __init__(self):\n",
    "        self.crawled_folder = Path('resources/crawled')\n",
    "        \n",
    "        # Load file_mapper for file paths\n",
    "        with open(self.crawled_folder / 'url_list.pickle', 'rb') as f:\n",
    "            self.file_mapper = pickle.load(f)\n",
    "        \n",
    "        # Initialize Elasticsearch client\n",
    "        self.es_client = Elasticsearch(\"https://localhost:9200\", \n",
    "                                       basic_auth=(\"elastic\", \"ySGo56ThrQ2moHl+WbG2\"), \n",
    "                                       ca_certs=\"~/http_ca.crt\")\n",
    "        \n",
    "        # Load PageRank instance\n",
    "        with open('pr_instance.pkl', 'rb') as f:\n",
    "            self.pr = pickle.load(f)\n",
    "\n",
    "    def run_indexer(self):\n",
    "        # Delete and create the index\n",
    "        self.es_client.options(ignore_status=[400, 404]).indices.delete(index='extend')\n",
    "        self.es_client.options(ignore_status=[400]).indices.create(index='extend')\n",
    "\n",
    "        for file in os.listdir(self.crawled_folder):\n",
    "            if file.endswith(\".txt\"):\n",
    "                j = json.load(open(os.path.join(self.crawled_folder, file)))\n",
    "                j['id'] = j['url']\n",
    "                \n",
    "                # Get PageRank score, ensure the key exists\n",
    "                pagerank_score = self.pr.pr_result.get(j['id'], {}).get('score', 0)\n",
    "                j['pagerank'] = pagerank_score\n",
    "\n",
    "                # Fetch TF-IDF score (Assume it's fetched via a search query)\n",
    "                search_result = self.es_client.search(index=\"extend\", body={\n",
    "                    \"query\": {\n",
    "                        \"match\": {\n",
    "                            \"url\": j['url']\n",
    "                        }\n",
    "                    },\n",
    "                    \"explain\": True  # Enable explanation to get TF-IDF calculation\n",
    "                })\n",
    "\n",
    "                # Extract TF-IDF score from the search explanation\n",
    "                if search_result['hits']['hits']:\n",
    "                    explain = search_result['hits']['hits'][0]['_explanation']\n",
    "                    tfidf_score = explain['value']\n",
    "                else:\n",
    "                    tfidf_score = 1  # Default value if no results are found\n",
    "\n",
    "                # Combine both TF-IDF and PageRank scores for final ranking\n",
    "                final_score = pagerank_score + tfidf_score\n",
    "                j['final_score'] = final_score\n",
    "\n",
    "                # Optionally, you can store both TF-IDF and PageRank scores separately for analysis\n",
    "                j['tfidf_score'] = tfidf_score\n",
    "\n",
    "                # Index the document with both scores\n",
    "                self.es_client.index(index='extend', body=j)\n",
    "                print(f\"Indexed {j['url']} with PageRank: {pagerank_score}, TF-IDF: {tfidf_score}, Final Score: {final_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "import time\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.es_client = Elasticsearch(\"https://localhost:9200\", basic_auth=(\"elastic\", \"ySGo56ThrQ2moHl+WbG2\"), ca_certs=\"~/http_ca.crt\")\n",
    "\n",
    "@app.route('/search_es_pr', methods=['GET'])\n",
    "def search_es_pr():\n",
    "    start = time.time()\n",
    "    response_object = {'status': 'success'}\n",
    "    \n",
    "    # Retrieve query term from request\n",
    "    argList = request.args.to_dict(flat=False)\n",
    "    query_term = argList['query'][0]\n",
    "    \n",
    "    # Elasticsearch query with explain to retrieve TF-IDF scoring\n",
    "    results = app.es_client.search(index='extend', source_excludes=['url_lists'], size=100,\n",
    "                                   query={\n",
    "                                       \"match\": {\n",
    "                                           \"text\": query_term\n",
    "                                       }\n",
    "                                   },\n",
    "                                   explain=True)  # Enable explanation to get detailed score breakdown\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    total_hit = results['hits']['total']['value']\n",
    "    \n",
    "    # Process the hits to extract the title, url, snippet, TF-IDF score and other details\n",
    "    results_df = pd.DataFrame([[\n",
    "        hit[\"_source\"]['title'], \n",
    "        hit[\"_source\"]['url'], \n",
    "        hit[\"_source\"]['text'][:100],  # Display first 100 characters of text\n",
    "        hit[\"_score\"],  # Use the score provided by Elasticsearch (which is now based on TF-IDF)\n",
    "        hit[\"_explanation\"]['value']  # Extract the TF-IDF score (or detailed explanation)\n",
    "    ] for hit in results['hits']['hits']], columns=['title', 'url', 'text', 'score', 'tfidf_score'])\n",
    "\n",
    "    # Prepare response data\n",
    "    response_object['total_hit'] = total_hit\n",
    "    response_object['results'] = results_df.to_dict('records')\n",
    "    response_object['elapse'] = end - start\n",
    "\n",
    "    return response_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [13/Feb/2025 23:57:05] \"GET /search_es_pr?query=camt HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
